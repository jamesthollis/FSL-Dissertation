{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the code for the models used in the dissertation project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to do is create the Twitter lexicon of for spell correction. The lexicon used is available and was developed by S. Rosenthal, P. Nakov, S. Kiritchenko, S. M. Mohammad, A. Ritter, and V. Stoyanov for their paper Semeval-2015 task 10: Sentiment analysis in Twitter. Proceedings of the ninth international workshop on Semantic Evaluation Exercises (SemEval-2015). 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def make_lexicon(lexicon_file):\n",
    "    twitter_lexicon = []\n",
    "    with open(lexicon_file) as lexicon:\n",
    "        lexicon_reader = csv.reader(lexicon, delimiter='\\t')\n",
    "        for line in lexicon_reader:\n",
    "            twitter_lexicon.append(line[1])\n",
    "    return twitter_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the tweets and perform the preprocessing that is implemented in the load_tweets.py file. The first time this is performed the data batches need unzipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_tweets\n",
    "import os\n",
    "\n",
    "# load training data\n",
    "training_filepath = \"./drive/My Drive/training_data/\"\n",
    "\n",
    "# first time running only, unzip each batch\n",
    "first_time = False\n",
    "if first_time:\n",
    "    for r, d, f in os.walk(training_filepath):\n",
    "        for filename in f:\n",
    "            batch_folder = os.path.join(training_filepath, filename[:-4])\n",
    "            os.mkdir(batch_folder)\n",
    "            if '.tgz' in filename:\n",
    "                load_tweets.unzip_batch(os.path.join(r, filename), batch_folder)\n",
    "                \n",
    "# initialise lists for storing the data                \n",
    "cd_training_data, cp_training_data, dp_training_data = [], [], []\n",
    "cd_training_labels, cp_training_labels, dp_training_labels = [], [], []\n",
    "\n",
    "# make the lexicon\n",
    "lexicon = make_lexicon('./twitter-lexicon.txt')\n",
    "\n",
    "# preprocess the data batch by batch\n",
    "for r, d, f in os.walk(training_filepath):\n",
    "    for batch in d:\n",
    "        batch_folder = os.path.join(r, batch)\n",
    "        cd_batch_data, cp_batch_data, dp_batch_data, cd_batch_labels, cp_batch_labels, dp_batch_labels = load_tweets.load_batch(batch_folder, \"drive/My Drive/training_data/anonymized_user_info_by_chunk_training.csv\", lexicon)\n",
    "        cd_training_data += cd_batch_data\n",
    "        cp_training_data += cp_batch_data\n",
    "        dp_training_data += dp_batch_data\n",
    "        cd_training_labels += cd_batch_labels\n",
    "        cp_training_labels += cp_batch_labels\n",
    "        dp_training_labels += dp_batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries for the remainder of the preprocessing and for building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Bidirectional, LSTM, Dropout, MaxPooling1D, Conv1D, TimeDistributed, concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps of the preprocessing is to tokenize the text, convert to sequences, and pad so all tweets are the same length. Then we have a structured dataset where each user has 3000 tweets and each tweet is the same length (we decide on token length of 30 due to the 140 character limit of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pad_data(data, labels):\n",
    "    \n",
    "    # Tokenize the tweets\n",
    "    tokenizer = Tokenizer()\n",
    "    max_length = 30\n",
    "    users_train_X = []\n",
    "    for user in data:\n",
    "        user_train_X = []\n",
    "        for (tweet, score) in user:\n",
    "            tokenizer.fit_on_texts(tweet)\n",
    "            user_train_X.append((tweet, score))\n",
    "        users_train_X.append(user_train_X)\n",
    "\n",
    "    # Convert text to sequences, then pad sequences    \n",
    "    train_X = []\n",
    "    for user in users_train_X:\n",
    "        raw_user_tweets = [x[0] for x in user]\n",
    "        user_tweets = []\n",
    "        temp = tokenizer.texts_to_sequences(raw_user_tweets)\n",
    "        train_X.append(pad_sequences(temp, maxlen=max_length))\n",
    "    word_index = tokenizer.word_index # keep the word index for creating the embedding layers\n",
    "    \n",
    "    train_y = labels\n",
    "    \n",
    "    return train_X, train_y, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object for building the baseline and few-shot learning models. The current FSL methos that is implemented is method 3, where the data is partitioned pseudo-randomly across tweets for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(object):\n",
    "    \n",
    "    # initialise the model with the training data and labels, and the word index\n",
    "    def __init__(self, model_name, train_X, train_y, word_index):\n",
    "        self.model_name = model_name\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.word_index = word_index\n",
    "        \n",
    "        \n",
    "    # custom embedding method, which uses the Twitter GloVE trained embeddings\n",
    "    def embed_twitter(self):\n",
    "        # create embedding dictionary\n",
    "        self.embeddings_index = {}\n",
    "        \n",
    "        # open twitter-trained word embeddings\n",
    "        txt = open('./drive/My Drive/embeddings/glove.twitter.27B.100d.txt', 'r')\n",
    "        for line in txt:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            self.embeddings_index[word] = coefs\n",
    "        txt.close()\n",
    "        \n",
    "        # create embedding matrix for words in the word index\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index)+1, 100))\n",
    "        for word, index in self.word_index.items():\n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[index] = embedding_vector    \n",
    "    \n",
    "    # Architecture of the baseline model and task module\n",
    "    # baseline takes 3000 tweets, task module takes 1500\n",
    "    def build_model(self, tweets=3000, pretrained=True):\n",
    "        # define the custom embedding layer \n",
    "        embedding_layer = Embedding(len(self.word_index)+1, 100, input_length=(30), weights=[self.embedding_matrix], trainable=True)\n",
    "      \n",
    "        # create the embedding branches, one for each tweet\n",
    "        embedding_inputs = []\n",
    "        embedding_outputs = []\n",
    "        for i in range(tweets):\n",
    "            branch_input = Input(shape=(30,))\n",
    "            branch_embedding = embedding_layer(branch_input)\n",
    "            embedding_inputs.append(branch_input)\n",
    "            embedding_outputs.append(branch_embedding)\n",
    "\n",
    "        # merge layer combining all embeddings\n",
    "        merged = concatenate(embedding_outputs)\n",
    "        # bi-directional LSTM layer with size 50\n",
    "        blstm = Bidirectional(LSTM(50))(merged)\n",
    "        # Dense output layer, uses sigmoid activation function for binary classification problem\n",
    "        out = Dense(1,  activation='sigmoid')(blstm)\n",
    "        \n",
    "        # Assemble the model layers and define inputs and output\n",
    "        self.model = Model(inputs=embedding_inputs, outputs=[out])\n",
    "        \n",
    "        # if pretraining used load the weights for the desired layers, by name\n",
    "        if pretrained == True:\n",
    "            self.model.load_weights('pretrained_weights.h5', by_name=True)\n",
    "            \n",
    "        # compile model, and get weights if desired\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.init_weights = self.model.get_weights()\n",
    "\n",
    "    # build the task module by calling the baseline method above with desired number of tweets\n",
    "    # number of tweets is 1500 for FSL methods 2 and 3, 3000 for method 1 - currently implemented for method 3\n",
    "    def build_fsl_task(self):\n",
    "        self.build_model(tweets=1500)\n",
    "\n",
    "    # build correction module\n",
    "    def build_fsl_correction(self, correction_size):\n",
    "        \n",
    "        # define the embedding layers\n",
    "        embedding_layer = Embedding(len(self.word_index)+1, 100, input_length=(30), weights=[self.embedding_matrix], trainable=True)\n",
    "      \n",
    "        # create the embedding branches, one for each tweet\n",
    "        embedding_inputs = []\n",
    "        embedding_outputs = []\n",
    "        for i in range(correction_size*3000):\n",
    "            branch_input = Input(shape=(30,))\n",
    "            branch_embedding = embedding_layer(branch_input)\n",
    "            embedding_inputs.append(branch_input)\n",
    "            embedding_outputs.append(branch_embedding)\n",
    "\n",
    "        # input the prediction from the task module\n",
    "        task_prediction = Input(shape=(1,))\n",
    "        task_dense = Dense(1, activation='relu')(task_prediction)\n",
    "\n",
    "        # merge all embeddings\n",
    "        merged = concatenate(embedding_outputs)\n",
    "        blstm = Bidirectional(LSTM(50))(merged)\n",
    "        dense = Dense(1,  activation='relu')(blstm)\n",
    "\n",
    "        # merge the BLSTM outputs with the task module prediction\n",
    "        task_outputs = [dense, task_dense]\n",
    "        merged2 = concatenate(task_outputs)\n",
    "        out = Dense(1, activation='linear')(merged2)\n",
    "\n",
    "        # dassemble model with inputs and outputs\n",
    "        all_inputs = embedding_inputs\n",
    "        all_inputs.append(task_prediction)\n",
    "        self.correction_model = Model(inputs=all_inputs, outputs=[out])\n",
    "        \n",
    "        # define stochastiv gradient descent optimiser and compile model\n",
    "        sgd = optimizers.SGD(lr=0.01, decay=1, momentum=0.9, nesterov=True)\n",
    "        self.correction_model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "        self.init_weights = self.correction_model.get_weights()\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # define callbacks for early stopping and reducing learning rate\n",
    "        callbacks = [keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, verbose=1, patience=4), \n",
    "                     keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)]\n",
    "        \n",
    "        print('Beginning model training.')\n",
    "        self.training = self.model.fit(self.train_X, self.train_y, batch_size=10, validation_split=0, epochs=10, callbacks=callbacks, verbose=1)        \n",
    "        \n",
    "        # saving model weights for pretraining or saving the model.\n",
    "        #self.model.save(self.model_name+'.h5')\n",
    "        #print('Model weights saved.')   \n",
    "        \n",
    "        \n",
    "    # Training the FSL model, train_X2 trains the task module, train_X3 trains the correction module\n",
    "    def train_fsl(self, train_X3, train_X2, train_y2):\n",
    "        \n",
    "        # define callbacks for early stopping and reducing learning rate\n",
    "        callbacks = [keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, verbose=1, patience=4), \n",
    "                    keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)]\n",
    "\n",
    "        \n",
    "        print('Training FSL Task Module')\n",
    "        self.training = self.model.fit(self.train_X, self.train_y, batch_size=5, validation_split=0, epochs=10, callbacks=callbacks, verbose=1)\n",
    "\n",
    "        # make the task predictions\n",
    "        task_predictions = self.model.predict(train_X2)\n",
    "\n",
    "        # calculate the true 'corrections' needed on the task predictions \n",
    "        # these are the training labels for the correction module\n",
    "        train_y_prime = np.asarray([float(train_y2[i]-task_predictions[i]) for i in range(len(train_y2))])\n",
    "        train_X2.append(task_predictions)\n",
    "        \n",
    "        # train the correction module with the calculated correction labels\n",
    "        print('Training FSL Correction Module')\n",
    "        self.training_2 = self.correction_model.fit(train_X3, train_y_prime, batch_size=5, validation_split=0, epochs=10, callbacks=callbacks, verbose=1)\n",
    "\n",
    "    # Get the training histories for the models\n",
    "    # baseline model has one history per training session, FSL has 2 - one for each module\n",
    "    def get_training(self, fsl=True):\n",
    "        if fsl:\n",
    "            return self.training, self.training_2\n",
    "        else:\n",
    "            return self.training\n",
    "\n",
    "    # make predictions for the test data   \n",
    "    def evaluate(self, test_X):\n",
    "        predictions = self.model.predict(test_X)\n",
    "        baseline_y_round = [1 if prediction>0.5 else 0 for prediction in predictions]# round using threshold 0.5\n",
    "        return baseline_y_round, predictions\n",
    "\n",
    "    # make predictions for the test data with the FSL model\n",
    "    def evaluate_fsl(self, test_X1, test_X2):\n",
    "        # make task predictions \n",
    "        task_predictions = self.model.predict(test_X1)\n",
    "        # add predictions to test data for correction module\n",
    "        test_X2.append(task_predictions)\n",
    "        # make correction prediction\n",
    "        correction_predictions = self.correction_model.predict(test_X2)\n",
    "\n",
    "        # assemble predictions by summing the task prediction and the correction\n",
    "        fsl_predictions = [task_predictions[i] + correction_predictions[i] for i in range(len(task_predictions))]\n",
    "        fsl_y = [1 if prediction>0.5 else 0 for prediction in fsl_predictions]# round using threshold 0.5\n",
    "        return fsl_y, fsl_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depression vs. Control sub-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the data, then set up the 5-fold cross validation for this task for the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, cd_word_index = load_pad_data(cd_training_data, cd_training_labels)\n",
    "\n",
    "training = []\n",
    "classification_reports = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "\n",
    "    # separate the test fold from the training data\n",
    "    length1 = int(0.2*(i-1)*len(train_X))\n",
    "    length2 = int(0.2*(i)*len(train_X))\n",
    "    if i == 1:\n",
    "        train_X1, test_X1 = train_X[length2:], train_X[:length2]\n",
    "        train_y1, test_y1 = train_y[length2:], train_y[:length2]\n",
    "    elif i == 5:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:]\n",
    "    else:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:length2]\n",
    "        train_X1 += train_X[length2:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:length2]\n",
    "        train_y1 += train_y[length2:]\n",
    "    \n",
    "    \n",
    "    train_X1 = [np.asarray(list(x)) for x in zip(*train_X1)]\n",
    "    train_y1 = np.asarray(train_y1)\n",
    "\n",
    "    test_X1 = [np.asarray(list(x)) for x in zip(*test_X1)]\n",
    "    test_y1 = np.asarray(test_y1)\n",
    "\n",
    "    # build model, create embedding and train model\n",
    "    cd_model = BLSTM('Baseline_cd{}'.format(str(i)), train_X1, train_y1, cd_word_index)\n",
    "    cd_model.embed_twitter()\n",
    "    cd_model.build_model()\n",
    "    cd_model.train()\n",
    "    \n",
    "    training.append(cd_model.get_training(fsl=False)) # save training history to list\n",
    "    \n",
    "    # get predictions on the test fold\n",
    "    rounded_predictions, predictions = cd_model.evaluate(test_X1)\n",
    "    classification_reports.append(classification_report(test_y1, rounded_predictions)) # get classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the classification reports help establish the performance of the model. Precision at 10% false alarm rates uses the unrounded predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process as for the baseline, but need to partition the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, cd_word_index = load_pad_data(cd_training_data, cd_training_labels)\n",
    "\n",
    "training = []\n",
    "classification_reports = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "\n",
    "    length1 = int(0.2*(i-1)*len(train_X))\n",
    "    length2 = int(0.2*(i)*len(train_X))\n",
    "    if i == 1:\n",
    "        train_X1, test_X1 = train_X[length2:], train_X[:length2]\n",
    "        train_y1, test_y1 = train_y[length2:], train_y[:length2]\n",
    "    elif i == 5:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:]\n",
    "    else:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:length2]\n",
    "        train_X1 += train_X[length2:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:length2]\n",
    "        train_y1 += train_y[length2:]\n",
    "    \n",
    "    train_X1 = [np.asarray(list(x)) for x in zip(*train_X1)]\n",
    "    train_y1 = np.asarray(train_y1)\n",
    "\n",
    "    test_X1 = [np.asarray(list(x)) for x in zip(*test_X1)]\n",
    "    test_y1 = np.asarray(test_y1)\n",
    "\n",
    "    # select indices for partitioning tweets\n",
    "    random_index = random.sample(range(len(train_X1)), int(0.5*len(train_X1)))\n",
    "    random_index.sort()\n",
    "\n",
    "    # partition tweets into two sets\n",
    "    train_X1, train_X2 = [train_X1[i] for i in random_index], [train_X1[i] for i in range(len(train_X1)) if i not in random_index]\n",
    "    test_X1, test_X2 = [test_X1[i] for i in random_index], [test_X1[i] for i in range(len(test_X1)) if i not in random_index]\n",
    "\n",
    "    cd_model = BLSTM('FSL_cd{}'.format(str(i)), train_X1, train_y1, cd_word_index)\n",
    "    cd_model.embed_twitter()\n",
    "    cd_model.build_fsl_task() # build task module\n",
    "    cd_model.build_fsl_correction() # build correction module\n",
    "    cd_model.train_fsl(train_X2, train_y1)\n",
    "    training.append(cd_model.get_training())\n",
    "    \n",
    "    rounded_predictions, predictions = cp_model.evaluate_fsl(test_X1, test_X2)\n",
    "    classification_reports.append(classification_report(test_y1, rounded_predictions)) # get classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTSD vs. Control sub-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, cp_word_index = load_pad_data(cp_training_data, cp_training_labels)\n",
    "\n",
    "training = []\n",
    "classification_reports = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "\n",
    "    # separate the test fold from the training data\n",
    "    length1 = int(0.2*(i-1)*len(train_X))\n",
    "    length2 = int(0.2*(i)*len(train_X))\n",
    "    if i == 1:\n",
    "        train_X1, test_X1 = train_X[length2:], train_X[:length2]\n",
    "        train_y1, test_y1 = train_y[length2:], train_y[:length2]\n",
    "    elif i == 5:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:]\n",
    "    else:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:length2]\n",
    "        train_X1 += train_X[length2:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:length2]\n",
    "        train_y1 += train_y[length2:]\n",
    "    \n",
    "    \n",
    "    train_X1 = [np.asarray(list(x)) for x in zip(*train_X1)]\n",
    "    train_y1 = np.asarray(train_y1)\n",
    "\n",
    "    test_X1 = [np.asarray(list(x)) for x in zip(*test_X1)]\n",
    "    test_y1 = np.asarray(test_y1)\n",
    "\n",
    "    # build model, create embedding and train model\n",
    "    cp_model = BLSTM('Baseline_cp{}'.format(str(i)), train_X1, train_y1, cp_word_index)\n",
    "    cp_model.embed_twitter()\n",
    "    cp_model.build_model()\n",
    "    cp_model.train()\n",
    "    \n",
    "    training.append(cp_model.get_training(fsl=False)) # save training history to list\n",
    "    \n",
    "    # get predictions on the test fold\n",
    "    rounded_predictions, predictions = cp_model.evaluate(test_X1)\n",
    "    classification_reports.append(classification_report(test_y1, rounded_predictions)) # get classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, cp_word_index = load_pad_data(cp_training_data, cp_training_labels)\n",
    "\n",
    "training = []\n",
    "classification_reports = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "\n",
    "    length1 = int(0.2*(i-1)*len(train_X))\n",
    "    length2 = int(0.2*(i)*len(train_X))\n",
    "    if i == 1:\n",
    "        train_X1, test_X1 = train_X[length2:], train_X[:length2]\n",
    "        train_y1, test_y1 = train_y[length2:], train_y[:length2]\n",
    "    elif i == 5:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:]\n",
    "    else:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:length2]\n",
    "        train_X1 += train_X[length2:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:length2]\n",
    "        train_y1 += train_y[length2:]\n",
    "    \n",
    "    train_X1 = [np.asarray(list(x)) for x in zip(*train_X1)]\n",
    "    train_y1 = np.asarray(train_y1)\n",
    "\n",
    "    test_X1 = [np.asarray(list(x)) for x in zip(*test_X1)]\n",
    "    test_y1 = np.asarray(test_y1)\n",
    "\n",
    "    # select indices for partitioning tweets\n",
    "    random_index = random.sample(range(len(train_X1)), int(0.5*len(train_X1)))\n",
    "    random_index.sort()\n",
    "\n",
    "    # partition tweets into two sets\n",
    "    train_X1, train_X2 = [train_X1[i] for i in random_index], [train_X1[i] for i in range(len(train_X1)) if i not in random_index]\n",
    "    test_X1, test_X2 = [test_X1[i] for i in random_index], [test_X1[i] for i in range(len(test_X1)) if i not in random_index]\n",
    "\n",
    "    cp_model = BLSTM('FSL_cp{}'.format(str(i)), train_X1, train_y1, cp_word_index)\n",
    "    cp_model.embed_twitter()\n",
    "    cp_model.build_fsl_task() # build task module\n",
    "    cp_model.build_fsl_correction() # build correction module\n",
    "    cp_model.train_fsl(train_X2, train_y1)\n",
    "    training.append(cp_model.get_training())\n",
    "    \n",
    "    rounded_predictions, predictions = cp_model.evaluate_fsl(test_X1, test_X2)\n",
    "    classification_reports.append(classification_report(test_y1, rounded_predictions)) # get classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depression vs. PTSD sub-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, dp_word_index = load_pad_data(dp_training_data, dp_training_labels)\n",
    "\n",
    "training = []\n",
    "classification_reports = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "\n",
    "    # separate the test fold from the training data\n",
    "    length1 = int(0.2*(i-1)*len(train_X))\n",
    "    length2 = int(0.2*(i)*len(train_X))\n",
    "    if i == 1:\n",
    "        train_X1, test_X1 = train_X[length2:], train_X[:length2]\n",
    "        train_y1, test_y1 = train_y[length2:], train_y[:length2]\n",
    "    elif i == 5:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:]\n",
    "    else:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:length2]\n",
    "        train_X1 += train_X[length2:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:length2]\n",
    "        train_y1 += train_y[length2:]\n",
    "    \n",
    "    \n",
    "    train_X1 = [np.asarray(list(x)) for x in zip(*train_X1)]\n",
    "    train_y1 = np.asarray(train_y1)\n",
    "\n",
    "    test_X1 = [np.asarray(list(x)) for x in zip(*test_X1)]\n",
    "    test_y1 = np.asarray(test_y1)\n",
    "\n",
    "    # build model, create embedding and train model\n",
    "    dp_model = BLSTM('Baseline_dp{}'.format(str(i)), train_X1, train_y1, dp_word_index)\n",
    "    dp_model.embed_twitter()\n",
    "    dp_model.build_model()\n",
    "    dp_model.train()\n",
    "    \n",
    "    training.append(dp_model.get_training(fsl=False)) # save training history to list\n",
    "    \n",
    "    # get predictions on the test fold\n",
    "    rounded_predictions, predictions = dp_model.evaluate(test_X1)\n",
    "    classification_reports.append(classification_report(test_y1, rounded_predictions)) # get classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, dp_word_index = load_pad_data(dp_training_data, dp_training_labels)\n",
    "\n",
    "training = []\n",
    "classification_reports = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "\n",
    "    length1 = int(0.2*(i-1)*len(train_X))\n",
    "    length2 = int(0.2*(i)*len(train_X))\n",
    "    if i == 1:\n",
    "        train_X1, test_X1 = train_X[length2:], train_X[:length2]\n",
    "        train_y1, test_y1 = train_y[length2:], train_y[:length2]\n",
    "    elif i == 5:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:]\n",
    "    else:\n",
    "        train_X1, test_X1 = train_X[:length1], train_X[length1:length2]\n",
    "        train_X1 += train_X[length2:]\n",
    "        train_y1, test_y1 = train_y[:length1], train_y[length1:length2]\n",
    "        train_y1 += train_y[length2:]\n",
    "    \n",
    "    train_X1 = [np.asarray(list(x)) for x in zip(*train_X1)]\n",
    "    train_y1 = np.asarray(train_y1)\n",
    "\n",
    "    test_X1 = [np.asarray(list(x)) for x in zip(*test_X1)]\n",
    "    test_y1 = np.asarray(test_y1)\n",
    "\n",
    "    # select indices for partitioning tweets\n",
    "    random_index = random.sample(range(len(train_X1)), int(0.5*len(train_X1)))\n",
    "    random_index.sort()\n",
    "\n",
    "    # partition tweets into two sets\n",
    "    train_X1, train_X2 = [train_X1[i] for i in random_index], [train_X1[i] for i in range(len(train_X1)) if i not in random_index]\n",
    "    test_X1, test_X2 = [test_X1[i] for i in random_index], [test_X1[i] for i in range(len(test_X1)) if i not in random_index]\n",
    "\n",
    "    dp_model = BLSTM('FSL_dp{}'.format(str(i)), train_X1, train_y1, dp_word_index)\n",
    "    dp_model.embed_twitter()\n",
    "    dp_model.build_fsl_task() # build task module\n",
    "    dp_model.build_fsl_correction() # build correction module\n",
    "    dp_model.train_fsl(train_X2, train_y1)\n",
    "    training.append(dp_model.get_training())\n",
    "    \n",
    "    rounded_predictions, predictions = dp_model.evaluate_fsl(test_X1, test_X2)\n",
    "    classification_reports.append(classification_report(test_y1, rounded_predictions)) # get classification report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
